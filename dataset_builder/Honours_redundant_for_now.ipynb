{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import shutil\n",
    "import pickle\n",
    "import sqlite3\n",
    "import IPython\n",
    "import fnmatch\n",
    "import copyreg\n",
    "import subprocess\n",
    "from math import exp\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def thread_it(thread_function, my_list, tq=True, WORKERS=None):\n",
    "    # Set worker number to CPU count\n",
    "    if not WORKERS:\n",
    "        WORKERS = multiprocessing.cpu_count()\n",
    "    \n",
    "    if tq:\n",
    "        tq = tqdm(total=len(my_list))\n",
    "    \n",
    "    # Separate into chunks and execute threaded\n",
    "    thread_list = chunks(my_list, WORKERS)\n",
    "    for chunk in thread_list:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "            for item in chunk:\n",
    "                executor.submit(thread_function, item)\n",
    "                if tq:\n",
    "                    tq.update(1)\n",
    "    tq.close()\n",
    "\n",
    "\n",
    "\n",
    "def thread_it_return(thread_function, my_list, tq=True, WORKERS=None):\n",
    "    # Set worker number to CPU count\n",
    "    if not WORKERS:\n",
    "        WORKERS = multiprocessing.cpu_count()\n",
    "    \n",
    "    if tq:\n",
    "        tq = tqdm(total=len(my_list))\n",
    "        \n",
    "    results = []\n",
    "    # Separate into chunks and execute threaded\n",
    "    thread_list = chunks(my_list, WORKERS)\n",
    "    for chunk in thread_list:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "            for item in chunk:\n",
    "                future = executor.submit(thread_function, item)\n",
    "                \n",
    "                return_value = future.result()\n",
    "                if return_value != None:\n",
    "                    results.append(return_value)\n",
    "                    \n",
    "                if tq:\n",
    "                    tq.update(1)\n",
    "    \n",
    "    tq.close()\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "def show_img_by_path(a, resize=True, size=(320, 240)):\n",
    "    # I could and probably need to implement image scaling beforehand, for network access\n",
    "    img = Image.open(a)\n",
    "    if resize:\n",
    "        img = img.resize(size=size)\n",
    "    IPython.display.display(img)\n",
    "\n",
    "\n",
    "def create_folder(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def move_to(file_list, dest):\n",
    "    tq = tqdm(total=len(file_list))\n",
    "    exception_flag = False\n",
    "    for item in file_list:\n",
    "        try:\n",
    "            shutil.move(item, dest)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            exception_flag = True\n",
    "        tq.update(1)\n",
    "    tq.close()\n",
    "    return exception_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define various variables\n",
    "#### This includes all paths for image folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should already exist from the \"Download\"\n",
    "compare_dir = \"compare_set/\"\n",
    "data_dir = \"images/\"\n",
    "points_num = 1\n",
    "\n",
    "consider_dir = \"consider/\"\n",
    "pickles = \"pickles/\"\n",
    "\n",
    "dirs = [consider_dir, pickles]\n",
    "for path in dirs:\n",
    "    create_folder(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction (Comparison set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = r\"colmap feature_extractor --database_path ./colmap_folder/colmap.db --image_path compare_set/ --SiftExtraction.max_num_features {}\".format(points_num)\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "print(output.splitlines()[-1])\n",
    "print(\"ERROR:\", error)\n",
    "print(\"Finished extracting features from Comparison set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching (Comparison set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = r\"colmap exhaustive_matcher --database_path ./colmap_folder/colmap.db\"\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "print(output.splitlines()[-1])\n",
    "print(\"ERROR:\", error)\n",
    "print(\"Finished matching features from Comparison set\")"
   ]
  },
  {
   "source": [
    "## Database Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file, timeout=30000)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn\n",
    "\n",
    "def delete_from(conn, table, column_name, where_value):\n",
    "    sql = r\"DELETE FROM {} WHERE {}='{}'\".format(table, column_name, where_value)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "def select_what_from_where(conn, what, table, where_name, where_value):\n",
    "    cur = conn.cursor()\n",
    "    row = cur.execute(r\"SELECT {} FROM {} WHERE {}='{}'\".format(what, table, column_name, where_value)).fetchone()\n",
    "    conn.commit()\n",
    "    return row\n",
    "\n",
    "def select_what_from(conn, what, table):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(r\"SELECT {} FROM {}\".format(what, table))\n",
    "    conn.commit()\n",
    "    rows = cur.fetchall()\n",
    "    conn.commit()\n",
    "    return rows\n",
    "\n",
    "def decrement_cameras(conn):\n",
    "    sql = r\"UPDATE sqlite_sequence SET seq = seq - 1 WHERE name='cameras'\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "def decrement_images(conn):\n",
    "    sql = r\"UPDATE sqlite_sequence SET seq = seq - 1 WHERE name='images'\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "def remove_img_from_db(conn, filename, compare_images, delete=False):\n",
    "    # Retrieve img_id and cam_id for image to delete\n",
    "    images_row = select_from_where(conn, \"images\", \"name\", filename)\n",
    "    img_id = images_row[0]\n",
    "    cam_id = images_row[2]\n",
    "\n",
    "    # Check if that's the only image referencing that camera.\n",
    "    images_rows = select_all(conn, \"images\")\n",
    "    only_cam_ref = True\n",
    "    for row in images_rows:\n",
    "        if row[2] == cam_id and row[0] != img_id:\n",
    "            # Cannot delete that camera\n",
    "            only_cam_ref = False\n",
    "\n",
    "    if only_cam_ref:\n",
    "        delete_from(conn, \"cameras\", \"camera_id\", cam_id)\n",
    "        decrement_cameras(conn)\n",
    "\n",
    "    # Delete any images, descriptors and keypoints for one data image record.\n",
    "    delete_from(conn, \"images\", \"name\", filename)\n",
    "    delete_from(conn, \"descriptors\", \"image_id\", img_id)\n",
    "    delete_from(conn, \"keypoints\", \"image_id\", img_id)\n",
    "\n",
    "\n",
    "    # Delete all matches and two_view_geometries for one data image to all comparison images\n",
    "    for compare_img in compare_images:\n",
    "        compare_filename = compare_img.split(\"/\")[-1]\n",
    "        try:\n",
    "            return_val = select_from_where(conn, \"images\", \"name\", compare_filename)\n",
    "            compare_img_id = return_val[0]\n",
    "        except Exception:\n",
    "            # If the return values is none, assume no matches for this image.\n",
    "            #print(compare_filename, return_val)\n",
    "            pass\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(img_id, compare_img_id)\n",
    "\n",
    "        delete_from(conn, \"matches\", \"pair_id\", pair_id)\n",
    "        delete_from(conn, \"two_view_geometries\", \"pair_id\", pair_id)\n",
    "\n",
    "    decrement_images(conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://colmap.github.io/database.html\n",
    "#https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "def load_matches(conn):\n",
    "    matches = {}\n",
    "    id_to_img = {}\n",
    "    for row in select_what_from(conn, \"image_id, name\", \"images\"):\n",
    "        img_id = row[0]\n",
    "        name = row[1]\n",
    "        id_to_img[img_id] = name\n",
    "\n",
    "    for img in id_to_img.values():\n",
    "        matches[img] = {}\n",
    "\n",
    "    for row in select_what_from(conn, \"pair_id, rows\", \"matches\"):\n",
    "        pair_id = row[0]\n",
    "        img1id, img2id = pair_id_to_image_ids(pair_id)\n",
    "        num_matches = row[1]\n",
    "        img1name, img2name = id_to_img[img1id], id_to_img[img2id]\n",
    "        matches[img1name][img2name] = num_matches\n",
    "        matches[img2name][img1name] = num_matches\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    return matches\n",
    "\n",
    "conn = create_connection(\"./colmap_folder/colmap.db\")\n",
    "compare_matches = load_matches(conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def total_matches(matches):\n",
    "    totals = {}\n",
    "    for key in matches.keys():\n",
    "        totals[key] = []\n",
    "    for key, value in matches.items():\n",
    "        length = len(value)\n",
    "        for _, num_matches in value.items():\n",
    "            totals[key].append(num_matches)\n",
    "    for key in totals.keys():\n",
    "        totals[key] = sum(totals[key])\n",
    "\n",
    "    return totals\n",
    "\n",
    "compare_total_matches = total_matches(compare_matches)\n",
    "# If one image doesn't match at all, either eliminate it (and rerun) or find more images for the set.\n",
    "pprint(compare_total_matches)\n",
    "\n",
    "for key, val in compare_total_matches.items():\n",
    "    if val == 0:\n",
    "        print(key, \"has no mathces.\")\n",
    "        #show_img_by_path(compare_dir+key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate threshold from comparison set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thr_from_compare(totals, multipler=1):\n",
    "    thr_per_image = {}\n",
    "    \n",
    "    tq = tqdm(total=len(totals))\n",
    "    for img1, item in totals.items():\n",
    "        for img2, val in item.items():\n",
    "            thr = val * multipler\n",
    "            \n",
    "            # Add thr to dict\n",
    "            if img2 not in thr_per_image.keys():\n",
    "                thr_per_image[img2] = [thr]\n",
    "            else:\n",
    "                thr_per_image[img2].append(thr)\n",
    "        tq.update(1)\n",
    "\n",
    "    # Get the average feature match for a valid image for each image in the reference set to every other image\n",
    "    for key in thr_per_image.keys():\n",
    "        val = thr_per_image[key]\n",
    "        thr_per_image[key] = sum(val)/len(val)\n",
    "        \n",
    "    return thr_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "thr_per_image = get_thr_from_compare(compare_matches, multipler=0.65)\n",
    "pprint(thr_per_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply threshold to data directory set function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_threshold_items(totals, thr_per_image, at_least_img_num=5, show=False):\n",
    "    values = {}\n",
    "    #x, y = [], []\n",
    "    tq = tqdm(total=len(totals))\n",
    "    for img1, item in totals.items():\n",
    "        rating = 0\n",
    "        \n",
    "        for img2, val in item.items():\n",
    "            # If the \"Data\" image is under the thr for the comparison image\n",
    "            if val > thr_per_image[img2]:\n",
    "                # Show which images from the comparison set, the data image is under thr for, and how much\n",
    "                rating += 1\n",
    "        \n",
    "        if rating >= at_least_img_num:\n",
    "            rating = 1\n",
    "        else:\n",
    "            rating = 0 \n",
    "        #rating = sigmoid(rating)\n",
    "\n",
    "        values[img1] = rating\n",
    "        \n",
    "        tq.update(1)\n",
    "    \"\"\"    \n",
    "    if show:\n",
    "        %matplotlib notebook\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(200, 200))\n",
    "        plt.plot(y, x, \"o\", color=\"black\")\n",
    "        plt.plot([x for x in range(len(x))], [confidence for x in range(len(x))], '-ok', color=\"red\")\n",
    "        plt.xlabel(\"Number of features\")\n",
    "        plt.ylabel(\"Confidence\")\n",
    "        plt.show()\n",
    "        print(\"Average is \", confidence)\n",
    "    \"\"\"\n",
    "        \n",
    "    return values"
   ]
  },
  {
   "source": [
    "## Calculating number of matches for each image (Data set) to entire comparison set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://colmap.github.io/database.html\n",
    "#https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "data_matches = {}\n",
    "conn = create_connection(\"./colmap_folder/colmap.db\")\n",
    "data_images = glob.glob(data_dir+\"*.jpg\")\n",
    "data_pair_matching = \"./colmap_folder/pairs_to_match.txt\"\n",
    "compare_ids_to_img_path = pickles+\"compare_ids_to_img.pickle\"\n",
    "comparison_pair_ids_path = pickles+\"comparison_pair_ids.pickle\"\n",
    "\n",
    "\n",
    "# Retrieve all img_ids and filenames for comparison set\n",
    "if os.path.isfile(compare_ids_to_img_path):\n",
    "    comparison_only_pair_ids = pickle.load(open(compare_ids_to_img_path, \"rb\"))\n",
    "    print(\"LOADED compare_ids_to_img FROM PICKLE\")\n",
    "else:\n",
    "    compare_ids_to_img = {}\n",
    "    return_val = select_what_from(conn, \"image_id, name\", \"images\")\n",
    "    for row in return_val:\n",
    "        compare_img_id = str(int(row[0]))\n",
    "        compare_img_filename = row[1]\n",
    "        compare_ids_to_img[compare_img_id] = compare_img_filename\n",
    "    pickle.dump(compare_ids_to_img, open(compare_ids_to_img_path, \"wb\"))\n",
    "\n",
    "print(\"Length of 'compare_ids_to_img':\", len(compare_ids_to_img.keys()))\n",
    "\n",
    "# Get all comparison only pair_ids\n",
    "if os.path.isfile(comparison_pair_ids_path):\n",
    "    comparison_only_pair_ids = pickle.load(open(comparison_pair_ids_path, \"rb\"))\n",
    "    print(\"LOADED comparison_only_pair_ids FROM PICKLE\")\n",
    "else:    \n",
    "    comparison_only_pair_ids = []\n",
    "    return_val = select_what_from(conn, \"pair_id\", \"matches\")\n",
    "    for row in return_val:\n",
    "        pair_id = str(int(row[0]))\n",
    "        comparison_only_pair_ids.append(pair_id)\n",
    "    pickle.dump(comparison_only_pair_ids, open(comparison_pair_ids_path, \"wb\"))\n",
    "\n",
    "print(\"Length of 'comparison_only_pair_ids':\", len(comparison_only_pair_ids))\n",
    "\n",
    "# Write all image pairings for each data image to each comparison image but not to other data images.\n",
    "for data_img in data_images:\n",
    "    data_filename = data_img.split(\"/\")[-1]\n",
    "\n",
    "    for compare_img in compare_ids_to_img.values():\n",
    "        #to_write = data_filename + \" \" + compare_img + \"\\n\"\n",
    "        to_write = data_filename + \" \" + compare_img + \"\\n\"\n",
    "        f = open(data_pair_matching, \"a\").write(to_write)\n",
    "\"\"\"\n",
    "cmd = r\"colmap feature_extractor --database_path ./colmap_folder/colmap.db --image_path ./images --SiftExtraction.max_num_features {}\".format(points_num)\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "p_status = process.wait()\n",
    "print(\"EXTRACTION ERROR: \", error)\n",
    "\"\"\"\n",
    "\n",
    "# Retrieve all img_ids and filenames for data set\n",
    "data_ids_to_img = {}\n",
    "return_val = select_what_from(conn, \"image_id, name\", \"images\")\n",
    "for row in return_val:\n",
    "    data_img_id = str(int(row[0]))\n",
    "    data_img_filename = row[1]\n",
    "    # If not a comparison image\n",
    "    if data_img_id not in compare_ids_to_img.keys():\n",
    "        data_ids_to_img[data_img_id] = data_img_filename\n",
    "\n",
    "# HAVE TO FUCKING CLOSE AND REOPEN THE DATABASE NOW!\n",
    "conn.close()\n",
    "\n",
    "\"\"\"\n",
    "# Match all image pairs for data set.\n",
    "cmd = r\"colmap matches_importer --database_path ./colmap_folder/colmap.db --match_list_path ./colmap_folder/pairs_to_match.txt --SiftMatching.max_num_matches 16384\"\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "p_status = process.wait()\n",
    "print(\"MATCHING ERROR: \", error)\n",
    "#print(output)\n",
    "\"\"\"\n",
    "# WILL NEED FOR REPEAT\n",
    "#os.remove(data_pair_matching)\n",
    "\n",
    "# REOPEN THAT SHIT\n",
    "conn = create_connection(\"./colmap_folder/colmap.db\")\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "@jit(parallel=True)\n",
    "def get_matches_from_db(return_val):\n",
    "    # Get all match numbers\n",
    "    matches = {}\n",
    "\n",
    "    # Prefill matches\n",
    "    for image in data_images:\n",
    "        img = image.split(\"/\")[-1]\n",
    "        matches[img] = {}\n",
    "        \n",
    "    tq = tqdm(total=len(return_val))\n",
    "    #print(compare_ids_to_img)\n",
    "    for row in return_val:\n",
    "        pair_id = str(int(row[0]))\n",
    "        num_matches = row[1]\n",
    "        #print(row)\n",
    "        if pair_id in comparison_only_pair_ids:\n",
    "            continue\n",
    "        else:\n",
    "            img1, img2 = pair_id_to_image_ids(int(pair_id))\n",
    "            img1, img2 = str(int(img1)), str(int(img2))\n",
    "            # img1 is comparison image\n",
    "            if img1 in compare_ids_to_img.keys():\n",
    "                img1 = compare_ids_to_img[img1]\n",
    "                img2 = data_ids_to_img[img2]\n",
    "                matches[img2][img1] = num_matches\n",
    "            # img2 is comparison image\n",
    "            elif img2 in compare_ids_to_img.keys():\n",
    "                img2 = compare_ids_to_img[img2]\n",
    "                img1 = data_ids_to_img[img1]\n",
    "                matches[img1][img2] = num_matches\n",
    "        tq.update(1)\n",
    "    return matches\n",
    "\n",
    "#pprint(matches)\n",
    "return_val = select_what_from(conn, \"pair_id, rows\", \"matches\")\n",
    "matches = get_matches_from_db(return_val)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = get_threshold_items(matches, thr_per_image, at_least_img_num=10, show=False)\n",
    "pprint(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pprint(ratings)\n",
    "under_confidence = []\n",
    "confidence = sum(ratings.values()) / len(ratings)\n",
    "print(\"CONFIDENCE: {}\".format(confidence))\n",
    "for key, val in ratings.items():\n",
    "    if val < confidence:\n",
    "        print(key, \"@\", val)\n",
    "        under_confidence.append(key)\n",
    "        #show_img_by_path(key, size=(75,75))\n",
    "\n",
    "print(\"{} out of {} images are under confident\".format(len(under_confidence), len(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def move_threshold_items(under, consider_folder, do_print=False):\n",
    "    for val in under:\n",
    "        val = data_dir+val\n",
    "        if os.path.isfile(val):\n",
    "            filename = val.split(\"/\")[-1]\n",
    "            path = os.path.join(consider_folder, filename)\n",
    "            try:\n",
    "                shutil.move(val, consider_folder)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(val, \"doesn't exist\")\n",
    "\n",
    "move_threshold_items(under_confidence, consider_dir)"
   ]
  },
  {
   "source": [
    "# An idea; I could possibly smush all the feature data for the comparison images into one object... That would give me a nice percentage as an overall match."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}