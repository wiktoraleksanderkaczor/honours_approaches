{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import os\n",
    "import sys \n",
    "import cv2\n",
    "import json \n",
    "import time\n",
    "import glob\n",
    "import shutil\n",
    "import pickle\n",
    "import IPython\n",
    "import hashlib\n",
    "import fnmatch\n",
    "import copyreg\n",
    "import requests\n",
    "import imagehash\n",
    "import flickrapi\n",
    "from math import exp\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 30 02:59:20 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1660    On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   36C    P8    11W / 130W |    899MiB /  5941MiB |      7%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1299      G   /usr/lib/xorg/Xorg                 41MiB |\r\n",
      "|    0   N/A  N/A      1906      G   /usr/lib/xorg/Xorg                694MiB |\r\n",
      "|    0   N/A  N/A      2032      G   /usr/bin/gnome-shell               43MiB |\r\n",
      "|    0   N/A  N/A      7303      G   ...AAAAAAAAA= --shared-files       93MiB |\r\n",
      "|    0   N/A  N/A     14355      G   ...token=2153280823035215176       14MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def thread_it(thread_function, my_list, tq=True, WORKERS=None):\n",
    "    # Set worker number to CPU count\n",
    "    if not WORKERS:\n",
    "        WORKERS = multiprocessing.cpu_count()\n",
    "    \n",
    "    if tq:\n",
    "        tq = tqdm(total=len(my_list))\n",
    "    \n",
    "    # Separate into chunks and execute threaded\n",
    "    thread_list = chunks(my_list, WORKERS)\n",
    "    for chunk in thread_list:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "            for item in chunk:\n",
    "                executor.submit(thread_function, item)\n",
    "                if tq:\n",
    "                    tq.update(1)\n",
    "                \n",
    "def thread_it_return(thread_function, my_list, tq=True, WORKERS=None):\n",
    "    # Set worker number to CPU count\n",
    "    if not WORKERS:\n",
    "        WORKERS = multiprocessing.cpu_count()\n",
    "    \n",
    "    if tq:\n",
    "        tq = tqdm(total=len(my_list))\n",
    "        \n",
    "    results = []\n",
    "    # Separate into chunks and execute threaded\n",
    "    thread_list = chunks(my_list, WORKERS)\n",
    "    for chunk in thread_list:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "            for item in chunk:\n",
    "                future = executor.submit(thread_function, item)\n",
    "                \n",
    "                return_value = future.result()\n",
    "                if return_value != None:\n",
    "                    results.append(return_value)\n",
    "                    \n",
    "                if tq:\n",
    "                    tq.update(1)\n",
    "    return results\n",
    "    \n",
    "\n",
    "def show_img(a):\n",
    "    IPython.display.display(Image.fromarray(a))\n",
    "    \n",
    "def show_img_by_path(a):\n",
    "    # I could and probably need to implement image scaling beforehand, for network access\n",
    "    IPython.display.display(Image.open(a))\n",
    "\n",
    "def create_folder(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link scraping (DuckDuckGo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(objs, exts):\n",
    "    links = []\n",
    "    for obj in objs:\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Width {0}, Height {1}\".format(obj[\"width\"], obj[\"height\"]))\n",
    "        print(\"Thumbnail {0}\".format(obj[\"thumbnail\"]))\n",
    "        print(\"Url {0}\".format(obj[\"url\"]))\n",
    "        print(\"Title {0}\".format(obj[\"title\"].encode('utf-8')))\n",
    "        print(\"Image {0}\".format(obj[\"image\"]))\n",
    "        \n",
    "        -- EXAMPLE OUTPUT --\n",
    "        Width 3840, Height 2560\n",
    "        Thumbnail https://tse1.mm.bing.net/th?id=OIF.BrhofaJg5Fx2yl9jrBBQLQ&pid=Api\n",
    "        Url https://www.airantares.ro/cazare/in-Paris/Franta/beaugrenelle-eiffel-tour-3-stars-paris-franta/\n",
    "        Title b'Beaugrenelle Tour Eiffel, Paris, Franta'\n",
    "        Image https://i.travelapi.com/hotels/2000000/1070000/1063000/1062936/c5a49732.jpg\n",
    "        \"\"\"\n",
    "\n",
    "        if (obj[\"width\"] * obj[\"height\"]) > 307200 and obj[\"image\"].split(\".\")[-1].lower() in exts:\n",
    "            links.append(obj[\"image\"])\n",
    "\n",
    "    return links\n",
    "\n",
    "def links_from_ddg(topic, max_images=None, exts=[\"jpg\", \"png\", \"bmp\", \"jpeg\"]):\n",
    "    link_list = []\n",
    "\n",
    "    url = 'https://duckduckgo.com/' \n",
    "    params = {'q': topic} \n",
    "\n",
    "    #   First make a request to above URL, and parse out the 'vqd'\n",
    "    #   This is a special token, which should be used in the subsequent request\n",
    "    res = requests.post(url, data=params)\n",
    "    searchObj = re.search(r'vqd=([\\d-]+)\\&', res.text, re.M|re.I) \n",
    "\n",
    "    if not searchObj:\n",
    "        # Token parsing failed\n",
    "        return -1 \n",
    "\n",
    "    headers = {\n",
    "        'authority': 'duckduckgo.com',\n",
    "        'accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'x-requested-with': 'XMLHttpRequest',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'referer': 'https://duckduckgo.com/',\n",
    "        'accept-language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('l', 'us-en'),\n",
    "        ('o', 'json'),\n",
    "        ('q', topic),\n",
    "        ('vqd', searchObj.group(1)),\n",
    "        ('f', ',,,'),\n",
    "        ('p', '1'),\n",
    "        ('v7exp', 'a'),\n",
    "    )\n",
    "\n",
    "    requestUrl = url + \"i.js\"\n",
    "\n",
    "    print(\"Scraping links from DuckDuckGo\")\n",
    "    tq = tqdm(total=max_images)\n",
    "    link_count = 0\n",
    "    while True:\n",
    "        while True:\n",
    "            try:\n",
    "                res = requests.get(requestUrl, headers=headers, params=params)\n",
    "                data = json.loads(res.text)\n",
    "                break\n",
    "            except ValueError:\n",
    "                # Hitting Url Failure - Sleep and Retry\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "        links = extract_json(data[\"results\"], exts)\n",
    "        for link in links:\n",
    "            if max_images and link_count != max_images:\n",
    "                link_list += [link]\n",
    "                link_count += 1\n",
    "                tq.update(1)\n",
    "            else:\n",
    "                return link_list\n",
    "            \n",
    "\n",
    "        if \"next\" not in data:\n",
    "            # No next page\n",
    "            return link_list\n",
    "\n",
    "        requestUrl = url + data[\"next\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link scraping (Flickr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_from_flickr(topic, max_images):\n",
    "    KEY = '88a8660edd2e770b1b00e878af174879'\n",
    "    SECRET = 'f3063c276e3ad859'\n",
    "\n",
    "    SIZES = [\"url_o\", \"url_k\", \"url_h\", \"url_l\", \"url_c\"]  # in order of preference\n",
    "\n",
    "    \"\"\"\n",
    "    - url_o: Original (4520 × 3229)\n",
    "    - url_k: Large 2048 (2048 × 1463)\n",
    "    - url_h: Large 1600 (1600 × 1143)\n",
    "    - url_l=: Large 1024 (1024 × 732)\n",
    "    - url_c: Medium 800 (800 × 572)\n",
    "    - url_z: Medium 640 (640 × 457)\n",
    "    - url_m: Medium 500 (500 × 357)\n",
    "    - url_n: Small 320 (320 × 229)\n",
    "    - url_s: Small 240 (240 × 171)\n",
    "    - url_t: Thumbnail (100 × 71)\n",
    "    - url_q: Square 150 (150 × 150)\n",
    "    - url_sq: Square 75 (75 × 75)\n",
    "    \"\"\"\n",
    "    \n",
    "    extras = ','.join(SIZES)\n",
    "    flickr = flickrapi.FlickrAPI(KEY, SECRET)\n",
    "    photos = flickr.walk(text=topic,  # it will search by image title and image tags\n",
    "                            extras=extras,  # get the urls for each size we want\n",
    "                            privacy_filter=1,  # search only for public photos\n",
    "                            per_page=50,\n",
    "                            sort='relevance')  # we want what we are looking for to appear first\n",
    "    counter, urls = 0, []\n",
    "\n",
    "    print(\"Scraping links from Flickr\")\n",
    "    tq = tqdm(total = max_images)\n",
    "    for photo in photos:\n",
    "        if counter < max_images:\n",
    "            for i in range(len(SIZES)):  # makes sure the loop is done in the order we want\n",
    "                url = photo.get(SIZES[i])\n",
    "                if url:  # if url is None try with the next size\n",
    "                    urls.append(url)\n",
    "                    counter += 1\n",
    "                    tq.update(1)\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_download(item):\n",
    "    link = item[\"link\"]\n",
    "    folder = item[\"folder\"]\n",
    "    service = item[\"service\"]\n",
    "    link_hash = str(hashlib.md5(link.encode(\"utf-8\")).hexdigest())\n",
    "    ext = link.split(\".\")[-1].lower()\n",
    "    fname = \"image_{}.{}\".format(link_hash, ext)\n",
    "    \n",
    "    path = os.path.join(folder, fname)\n",
    "\n",
    "    if not os.path.isfile(path):\n",
    "        myfile = None\n",
    "        if service == \"ddg\":\n",
    "            myfile = requests.get(link, allow_redirects=True)\n",
    "        elif service == \"flickr\":\n",
    "            myfile = requests.get(link, stream=True)\n",
    "        \n",
    "        open(path, 'wb').write(myfile.content)        \n",
    "\n",
    "def download(links, folder, service=\"flickr\"):\n",
    "    items = []\n",
    "    for link in links:\n",
    "        items.append({\"link\": link, \"folder\": folder, \"service\": service})\n",
    "    thread_it(thread_download, items, WORKERS=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define various variables\n",
    "#### This includes all paths for image folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task\n",
    "compare_dir = \"compare_set/\"\n",
    "data_dir = \"images/\"\n",
    "consider_dir = \"consider/\"\n",
    "\n",
    "dirs = [compare_dir, data_dir, consider_dir]\n",
    "for path in dirs:\n",
    "    create_folder(path)\n",
    "\n",
    "CPUs = multiprocessing.cpu_count()\n",
    "data_num = 3000\n",
    "compare_num = 100\n",
    "topic = \"eiffel tower\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the sets\n",
    "#### Download the comparison set and count number of downloaded files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping links from DuckDuckGo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cd97eba7ed4dbf87459d27566a8faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832e4e4700cc4a61a1ff0df2047a3f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 172 images for the comparison set\n"
     ]
    }
   ],
   "source": [
    "# Download comparison set\n",
    "exts = [\"jpg\", \"jpeg\"]\n",
    "links = links_from_ddg(topic, max_images = compare_num, exts = exts)\n",
    "download(links, compare_dir, service=\"ddg\")\n",
    "file_num = len(glob.glob(compare_dir+\"*\", recursive=True))\n",
    "print(\"Downloaded {} images for the comparison set\".format(file_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data set and count number of downloaded files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping links from Flickr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eebaed5d7304247913bcf982db5e509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5171fbfd33ef459b89f746f79b232383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2979 images for the data set\n"
     ]
    }
   ],
   "source": [
    "# Download data set\n",
    "links = links_from_flickr(topic, max_images=data_num)\n",
    "download(links, data_dir, service=\"flickr\")\n",
    "file_num = len(glob.glob(data_dir+\"*\", recursive=True))\n",
    "print(\"Downloaded {} images for the data set\".format(file_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset formats\n",
    "#### Rename all files to \".jpg\" file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_rename(image):\n",
    "    fname = image.split(\".\")[-2]\n",
    "    ext = image.split(\".\")[-1]\n",
    "    if ext == \"jpeg\":\n",
    "        shutil.move(image, fname+\".jpg\")\n",
    "        \n",
    "def images_rename(path):\n",
    "    files = glob.glob(path+\"*.jpeg\", recursive=True)\n",
    "    thread_it(img_rename, files)\n",
    "\n",
    "images_rename(compare_dir)\n",
    "images_rename(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Detect corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "def verify_thread(image):\n",
    "    try:\n",
    "        img = io.imread(image)\n",
    "    except Exception as e:\n",
    "        return (image, e)\n",
    "\n",
    "def verify_images(path):\n",
    "    files = glob.glob(path+\"*.jpg\", recursive=True)\n",
    "    corrupt = thread_it_return(verify_thread, files)\n",
    "    return corrupt\n",
    "\n",
    "corrupt_compare = verify_images(compare_dir)\n",
    "print(\"Compare directory:\")\n",
    "pprint(corrupt_compare)\n",
    "\n",
    "corrupt_data = verify_images(data_dir)\n",
    "print(\"Data directory:\")\n",
    "pprint(corrupt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_remove_thread(item):\n",
    "    os.remove(item[0])\n",
    "\n",
    "def remove_corrupt(items):\n",
    "    thread_it(img_remove_thread, items)\n",
    "    \n",
    "remove_corrupt(corrupt_compare)\n",
    "remove_corrupt(corrupt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### Get blur variance average for the comparison set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_avg_thread(image):\n",
    "    img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "    val = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "    return val \n",
    "\n",
    "def get_blur_average(path, multiplier=1):\n",
    "    files = glob.glob(path+\"*.jpg\", recursive=True)\n",
    "    thr = thread_it_return(blur_avg_thread, files)\n",
    "    avg = sum(thr) / len(files)\n",
    "    return avg * multiplier\n",
    "\n",
    "blur_avg = get_blur_average(compare_dir, multiplier=0.75)\n",
    "print(\"The blur average is: \", blur_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Get the images in the data set that are more blurry than a given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def too_blurry_thread(item):\n",
    "    image = item[\"image\"]\n",
    "    threshold = item[\"threshold\"]\n",
    "    img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "    val = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "    if val < threshold:\n",
    "        return image\n",
    "                \n",
    "def get_too_blurry(path, threshold):\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "    items = []\n",
    "    for image in files:\n",
    "        items.append({\"image\": image, \"threshold\": threshold})\n",
    "    too_blurry = thread_it_return(too_blurry_thread, items)\n",
    "    \n",
    "    print(\"{} out of {} images are blurry\".format(len(too_blurry), len(files)))\n",
    "    return too_blurry\n",
    "\n",
    "too_blurry = get_too_blurry(data_dir, blur_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the files referred to by the paths in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from_list(file_list):\n",
    "    tq = tqdm(total=len(file_list))\n",
    "    for item in file_list:\n",
    "        os.remove(item)\n",
    "        tq.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_from_list(too_blurry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the maximum number of pixels in any image within the comparison and data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_resolution_thread(image):\n",
    "    img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Get image height and width\n",
    "    #height, width, channels = img.shape\n",
    "    height, width = img.shape\n",
    "\n",
    "    # Count maximum resolution\n",
    "    val = width * height\n",
    "    \n",
    "    # Show warning if problem\n",
    "    if val < 307200:\n",
    "        print(\"{} is too small \\({}\\)\".format(image, val))\n",
    "    \n",
    "    return val\n",
    "\n",
    "def get_max_resolution(path):\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "    max_resolution = thread_it_return(max_resolution_thread, files)\n",
    "    \n",
    "    return max(max_resolution)\n",
    "\n",
    "\n",
    "max_resolution_compare = get_max_resolution(compare_dir)\n",
    "print(\"Max resolution for compare: \", max_resolution_compare)\n",
    "max_resolution_data = get_max_resolution(data_dir)\n",
    "print(\"Max resolution for data: \", max_resolution_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "#### Hashing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_thread(image):\n",
    "    img_hash = imagehash.dhash(Image.open(image))\n",
    "    fname = image.split(\".\")[-2]+\".hash\"\n",
    "    if not os.path.isfile(fname):\n",
    "        pickle.dump(img_hash, open(fname, \"wb\"))    \n",
    "\n",
    "def compute_img_hashes(path):\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "    thread_it(hash_thread, files)\n",
    "\n",
    "compute_img_hashes(compare_dir)\n",
    "compute_img_hashes(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute hash distances for each image to each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_distance_thread(item):\n",
    "    hash1 = pickle.load(open(item[\"hashname\"], \"rb\"))\n",
    "    dis = item[\"hashname\"].split(\".\")[-2]+\".dis\"\n",
    "    check = item[\"hashname\"].split(\".\")[-2]+\".jpg\"\n",
    "    \n",
    "    if os.path.isfile(dis):\n",
    "        compute = pickle.load(open(dis, \"rb\"))\n",
    "    else:\n",
    "        compute = {}\n",
    "    \n",
    "    for hashpath in item[\"files\"]:\n",
    "        try:\n",
    "            image = hashpath.split(\".\")[-2]+\".jpg\"\n",
    "            if image not in compute.keys() and image != check:\n",
    "                hash2 = pickle.load(open(hashpath, \"rb\"))\n",
    "                compute[image] = hash1 - hash2\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    pickle.dump(compute, open(dis, \"wb\"))\n",
    "\n",
    "def compute_hash_distance(path):\n",
    "    files = glob.glob(path+\"*.hash\")\n",
    "    \n",
    "    items = []\n",
    "    for image in files:\n",
    "        items.append({\"hashname\": image, \"files\": files})\n",
    "\n",
    "    thread_it(hash_distance_thread, items)\n",
    "\n",
    "compute_hash_distance(compare_dir)    \n",
    "compute_hash_distance(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicate_images(path, threshold=10):\n",
    "    files = glob.glob(path+\"*.dis\")\n",
    "    dup, close = [], []\n",
    "    \n",
    "    for path in files:\n",
    "        distances = pickle.load(open(path, \"rb\"))\n",
    "        img = path.split(\".\")[-2]+\".jpg\"\n",
    "        \n",
    "        for key, val in distances.items():\n",
    "            if val == 0:\n",
    "                if key not in dup and img not in dup:\n",
    "                    dup.append(key)\n",
    "            elif val < threshold:\n",
    "                if key not in close and img not in close:\n",
    "                    close.append(key)\n",
    "                \n",
    "    return dup, close\n",
    "\n",
    "compare_dup, compare_close = get_duplicate_images(compare_dir)\n",
    "data_dup, data_close = get_duplicate_images(data_dir)\n",
    "\n",
    "print(\"Compare duplicates:\")\n",
    "pprint(compare_dup)\n",
    "print(\"Compare close:\")\n",
    "pprint(compare_close)\n",
    "\n",
    "print(\"Data duplicates:\")\n",
    "pprint(data_dup)\n",
    "print(\"Data close:\")\n",
    "pprint(data_close)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    for item in duplicates:\n",
    "        show_img_by_path(item)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_from_list(compare_dup)\n",
    "remove_from_list(data_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Pickle behaviour for feature points.\n",
    "def _pickle_keypoints(point):\n",
    "    return cv2.KeyPoint, (*point.pt, point.size, point.angle,\n",
    "                          point.response, point.octave, point.class_id)\n",
    "\n",
    "# Register pickle handler for KeyPoints\n",
    "copyreg.pickle(cv2.KeyPoint().__class__, _pickle_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_thread(item):\n",
    "    image = item[\"image\"]\n",
    "    points = image.split(\".\")[-2]+\".pts\"\n",
    "    detector = item[\"detector\"]\n",
    "\n",
    "    if not os.path.isfile(points):\n",
    "        try:\n",
    "            img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "            kp, des = detector.detectAndCompute(img, None)\n",
    "            data = {\"kp\": kp, \"des\": des}\n",
    "            pickle.dump(data, open(points, \"wb\"))\n",
    "        except Exception as e:\n",
    "            print(\"{} failed because {}\".format(image, e))\n",
    "\n",
    "def feature_extraction(path, feature_matcher=\"ORB\", points_num=8192):\n",
    "    # Get all images list.\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "\n",
    "    detector = None\n",
    "    if feature_matcher == \"SIFT\":\n",
    "        detector = cv2.SIFT_create(nfeatures=points_num)\n",
    "    elif feature_matcher == \"SURF\":\n",
    "        detector = cv2.SURF_create(nfeatures=points_num)\n",
    "    elif feature_matcher == \"ORB\":\n",
    "        detector = cv2.ORB_create(nfeatures=points_num)\n",
    "\n",
    "    print(detector)\n",
    "        \n",
    "    items = []\n",
    "    for image in files:\n",
    "        items.append({\"image\": image, \"detector\": detector})\n",
    "    \n",
    "    thread_it(extract_thread, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "points_num = 2048\n",
    "features = \"SIFT\"\n",
    "feature_extraction(compare_dir, feature_matcher=features, points_num=points_num)\n",
    "feature_extraction(data_dir, feature_matcher=features, points_num=points_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matcher(feature_matcher=\"ORB\", bf_or_flann=\"BF\"):\n",
    "    #https://www.programcreek.com/python/?code=NetEase%2Fairtest%2Fairtest-master%2Fairtest%2Ftrash%2Ffind_obj.py\n",
    "    matcher, norm = None, None\n",
    "    if feature_matcher == \"ORB\":\n",
    "        norm = cv2.NORM_HAMMING\n",
    "    elif feature_matcher in [\"SIFT\", \"SURF\"]:\n",
    "        norm = cv2.NORM_L2\n",
    "    if bf_or_flann == \"FLANN\":\n",
    "        if norm == cv2.NORM_L2:\n",
    "            FLANN_INDEX_KDTREE = 1\n",
    "            flann_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        else:\n",
    "            FLANN_INDEX_LSH = 6\n",
    "            flann_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                                table_number = 6, # 12\n",
    "                                key_size = 12,     # 20\n",
    "                                multi_probe_level = 1) #2\n",
    "        matcher = cv2.FlannBasedMatcher(flann_params, {})\n",
    "        # bug : need to pass empty dict (#1329)\n",
    "    elif bf_or_flann == \"BF\":\n",
    "        matcher = cv2.BFMatcher(norm)\n",
    "    else:\n",
    "        matcher = cv2.BFMatcher(norm)\n",
    "\n",
    "    return matcher\n",
    "\n",
    "\n",
    "def match_within_path(path, matcher, ratio_test=False):\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "    \n",
    "    tq = tqdm(total=len(files))\n",
    "    for img1 in files:\n",
    "        base1 = img1.split(\".\")[-2]\n",
    "        jpg1 = base1+\".jpg\"\n",
    "        features1 = pickle.load(open(base1+\".pts\", \"rb\"))\n",
    "        \n",
    "        if os.path.isfile(base1+\".mch\"):\n",
    "            matches1 = pickle.load(open(base1+\".mch\", \"rb\"))\n",
    "        else:\n",
    "            matches1 = {}\n",
    "            \n",
    "        for img2 in files:\n",
    "            # Skip if same image\n",
    "            if img1 is img2:\n",
    "                continue\n",
    "            \n",
    "            base2 = img2.split(\".\")[-2]\n",
    "            jpg2 = base2+\".jpg\"\n",
    "            features2 = pickle.load(open(base2+\".pts\", \"rb\"))\n",
    "            \n",
    "            # If either were matched against the other, fill out and skip\n",
    "            if os.path.isfile(base2+\".mch\"):\n",
    "                matches2 = pickle.load(open(base2+\".mch\", \"rb\"))\n",
    "                if jpg1 in matches2.keys():\n",
    "                    matches1[jpg2] = matches2[jpg1]\n",
    "                    continue\n",
    "                elif jpg2 in matches1.keys():\n",
    "                    matches2[jpg1] = matches1[jpg2]\n",
    "                    continue\n",
    "            else:\n",
    "                matches2 = {}\n",
    "\n",
    "            # Read computed data.\n",
    "            des1 = features1[\"des\"]  # Actual set image\n",
    "            des2 = features2[\"des\"]  # Compare set image\n",
    "            try:\n",
    "                matches_data = matcher.knnMatch(des1, des2, k=2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                matches_data = []\n",
    "\n",
    "            if ratio_test:\n",
    "                # Apply ratio test\n",
    "                good = []\n",
    "                for m, n in matches_data:\n",
    "                    if m.distance < 0.75*n.distance:\n",
    "                        good.append(True)\n",
    "\n",
    "                matches1[jpg2] = len(good)\n",
    "                matches2[jpg1] = len(good)\n",
    "            else:\n",
    "                # Those that don't exist in here probably don't have matches can be removed\n",
    "                matches1[jpg2] = len(matches_data)\n",
    "                matches2[jpg1] = len(matches_data)\n",
    "\n",
    "            pickle.dump(matches2, open(base2+\".mch\", \"wb\"))\n",
    "        pickle.dump(matches1, open(base1+\".mch\", \"wb\"))\n",
    "        tq.update(1)\n",
    "\n",
    "\n",
    "def match_features_other_path(path, other_path, matcher, ratio_test=False):\n",
    "    # Get pre-computed images list.\n",
    "    files = glob.glob(path+\"*.pts\")\n",
    "    other = glob.glob(other_path+\"*.pts\")\n",
    "\n",
    "    tq = tqdm(total=len(files))\n",
    "    for img1 in files:\n",
    "        base1 = img1.split(\".\")[-2]\n",
    "        jpg1 = base1+\".jpg\"\n",
    "        \n",
    "        features1 = pickle.load(open(base1+\".pts\", \"rb\"))\n",
    "        if os.path.isfile(base1+\".mch\"):\n",
    "            matches = pickle.load(open(base1+\".mch\", \"rb\"))\n",
    "        else:\n",
    "            matches = {}\n",
    "            \n",
    "        for img2 in other:\n",
    "            # Skip if same image\n",
    "            if img1 is img2:\n",
    "                continue\n",
    "            \n",
    "            base2 = img2.split(\".\")[-2]\n",
    "            jpg2 = base2+\".jpg\"\n",
    "            \n",
    "            if jpg2 in matches.keys():\n",
    "                continue\n",
    "            \n",
    "            features2 = pickle.load(open(base2+\".pts\", \"rb\"))\n",
    "\n",
    "            # Read computed data.\n",
    "            des1 = features1[\"des\"]  # Actual set image\n",
    "            des2 = features2[\"des\"]  # Compare set image\n",
    "            try:\n",
    "                matches_data = matcher.knnMatch(des1, des2, k=2)\n",
    "            except Exception:\n",
    "                matches_data = []\n",
    "\n",
    "            if ratio_test:\n",
    "                # Apply ratio test\n",
    "                good = []\n",
    "                for m, n in matches_data:\n",
    "                    if m.distance < 0.75*n.distance:\n",
    "                        good.append(True)\n",
    "                        \n",
    "                matches[jpg2] = len(good)\n",
    "            else:\n",
    "                # Those that don't exist in here probably don't have matches can be removed\n",
    "                matches[jpg2] = len(matches_data)\n",
    "        tq.update(1)\n",
    "                \n",
    "\n",
    "        pickle.dump(matches, open(base1+\".mch\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = get_matcher(feature_matcher=features, bf_or_flann=\"BF\")\n",
    "print(matcher)\n",
    "\n",
    "match_within_path(compare_dir, matcher, ratio_test=True)\n",
    "match_features_other_path(data_dir, compare_dir, matcher, ratio_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_and_total_matches(path):\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "    matches = {}\n",
    "    for path in files:\n",
    "        match_path = path.split(\".\")[-2]+\".mch\"\n",
    "        matches[path] = pickle.load(open(match_path, \"rb\"))\n",
    "    return matches\n",
    "\n",
    "compare_matches = load_and_total_matches(compare_dir)\n",
    "data_matches = load_and_total_matches(data_dir)\n",
    "\n",
    "pprint(data_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def total_matches(matches, do_print=False):\n",
    "    totals = {}\n",
    "    for key in matches.keys():\n",
    "        totals[key] = []\n",
    "    for key, value in matches.items():\n",
    "        length = len(value)\n",
    "        for _, num_matches in value.items():\n",
    "            totals[key].append(num_matches)\n",
    "    for key in totals.keys():\n",
    "        totals[key] = sum(totals[key])\n",
    "\n",
    "    if do_print:\n",
    "        pprint(totals)\n",
    "    return totals\n",
    "\n",
    "compare_total_matches = total_matches(compare_matches)\n",
    "data_total_matches = total_matches(data_matches)\n",
    "\n",
    "pprint(compare_total_matches)\n",
    "pprint(data_total_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate threshold from comparison set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thr_from_compare(path, totals, multipler=1):\n",
    "    thr_per_image = {}\n",
    "    \n",
    "    tq = tqdm(total=len(totals))\n",
    "    for img1, item in totals.items():\n",
    "        for img2, val in item.items():\n",
    "            # Actual resolutions differences won't matter when using Scale-Invariant feature descriptions\n",
    "            # Only do multipler because references are the best case, real data won't be.\n",
    "            thr = val * multipler\n",
    "            \n",
    "            # Add thr to dict\n",
    "            if img2 not in thr_per_image.keys():\n",
    "                thr_per_image[img2] = [thr]\n",
    "            else:\n",
    "                thr_per_image[img2].append(thr)\n",
    "        tq.update(1)\n",
    "\n",
    "    # Get the average feature match for a valid image for each image in the reference set to every other image\n",
    "    for key in thr_per_image.keys():\n",
    "        val = thr_per_image[key]\n",
    "        thr_per_image[key] = sum(val)/len(val)\n",
    "        \n",
    "    return thr_per_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Does Lowe ratio apply here?\n",
    "#thr = thr*0.75\n",
    "\n",
    "thr_per_image = get_thr_from_compare(compare_dir, compare_matches, multipler=0.85)\n",
    "print(thr_per_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply threshold to data directory set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + exp(-x))\n",
    "\n",
    "\n",
    "def get_threshold_items(totals, thr_per_image, show=False):\n",
    "    values = {}\n",
    "    #x, y = [], []\n",
    "    tq = tqdm(total=len(totals))\n",
    "    for img1, item in totals.items():\n",
    "        # Get resolution\n",
    "        img = cv2.imread(img1, cv2.IMREAD_GRAYSCALE)\n",
    "        height, width = img.shape\n",
    "        res = width * height\n",
    "\n",
    "        rating = 0\n",
    "        \n",
    "        for img2, val in item.items():\n",
    "            # If the \"Data\" image is under the thr for the comparison image\n",
    "            if val > thr_per_image[img2]:\n",
    "                # Show which images from the comparison set, the data image is under thr for, and how much\n",
    "                rating += 1\n",
    "                \n",
    "        rating = rating / len(item) \n",
    "        #rating = sigmoid(rating)\n",
    "\n",
    "        values[img1] = rating\n",
    "        \n",
    "        tq.update(1)\n",
    "    \"\"\"    \n",
    "    if show:\n",
    "        %matplotlib notebook\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(200, 200))\n",
    "        plt.plot(y, x, \"o\", color=\"black\")\n",
    "        plt.plot([x for x in range(len(x))], [confidence for x in range(len(x))], '-ok', color=\"red\")\n",
    "        plt.xlabel(\"Number of features\")\n",
    "        plt.ylabel(\"Confidence\")\n",
    "        plt.show()\n",
    "        print(\"Average is \", confidence)\n",
    "    \"\"\"\n",
    "        \n",
    "    return values\n",
    "\n",
    "ratings = get_threshold_items(data_matches, thr_per_image, show=False)\n",
    "#pprint(ratings)\n",
    "\n",
    "under_confidence = []\n",
    "confidence = sum(ratings.values()) / len(ratings)\n",
    "print(\"CONFIDENCE: {}\".format(confidence))\n",
    "for key, val in ratings.items():\n",
    "    if val < confidence:\n",
    "        print(key, \"@\", val, \":\")\n",
    "        under_confidence.append(key)\n",
    "        #show_img_by_path(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_threshold_items(under, consider_folder, do_print=False):\n",
    "    for val in under:\n",
    "        if os.path.isfile(val):\n",
    "            filename = val.split(\"/\")[-1]\n",
    "            path = os.path.join(consider_folder, filename)\n",
    "            try:\n",
    "                shutil.move(val, consider_folder)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(val, \"doesn't exist\")\n",
    "\n",
    "move_threshold_items(under_confidence, consider_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
