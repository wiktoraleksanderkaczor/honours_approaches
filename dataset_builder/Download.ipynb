{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbasecondad5f027570b844e40ba6c6d5b4044b6f5",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import wget\n",
    "import json\n",
    "import glob\n",
    "import urllib\n",
    "import hashlib\n",
    "import requests\n",
    "import flickrapi\n",
    "import posixpath\n",
    "from skimage import io\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "source": [
    "## Helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def thread_it(thread_function, my_list, tq=True, WORKERS=None):\n",
    "    # Set worker number to CPU count\n",
    "    if not WORKERS:\n",
    "        WORKERS = multiprocessing.cpu_count()\n",
    "    \n",
    "    if tq:\n",
    "        tq = tqdm(total=len(my_list))\n",
    "    \n",
    "    # Separate into chunks and execute threaded\n",
    "    thread_list = chunks(my_list, WORKERS)\n",
    "    for chunk in thread_list:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "            for item in chunk:\n",
    "                executor.submit(thread_function, item)\n",
    "                if tq:\n",
    "                    tq.update(1)\n",
    "    tq.close()\n",
    "\n",
    "\n",
    "def thread_it_return(thread_function, my_list, tq=True, WORKERS=None):\n",
    "    # Set worker number to CPU count\n",
    "    if not WORKERS:\n",
    "        WORKERS = multiprocessing.cpu_count()\n",
    "    \n",
    "    if tq:\n",
    "        tq = tqdm(total=len(my_list))\n",
    "        \n",
    "    results = []\n",
    "    # Separate into chunks and execute threaded\n",
    "    thread_list = chunks(my_list, WORKERS)\n",
    "    for chunk in thread_list:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "            for item in chunk:\n",
    "                future = executor.submit(thread_function, item)\n",
    "                \n",
    "                return_value = future.result()\n",
    "                if return_value != None:\n",
    "                    results.append(return_value)\n",
    "                    \n",
    "                if tq:\n",
    "                    tq.update(1)\n",
    "    \n",
    "    tq.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_folder(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "source": [
    "## Link scraping (DuckDuckGo)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(objs, exts):\n",
    "    links = []\n",
    "    for obj in objs:\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Width {0}, Height {1}\".format(obj[\"width\"], obj[\"height\"]))\n",
    "        print(\"Thumbnail {0}\".format(obj[\"thumbnail\"]))\n",
    "        print(\"Url {0}\".format(obj[\"url\"]))\n",
    "        print(\"Title {0}\".format(obj[\"title\"].encode('utf-8')))\n",
    "        print(\"Image {0}\".format(obj[\"image\"]))\n",
    "        \n",
    "        -- EXAMPLE OUTPUT --\n",
    "        Width 3840, Height 2560\n",
    "        Thumbnail https://tse1.mm.bing.net/th?id=OIF.BrhofaJg5Fx2yl9jrBBQLQ&pid=Api\n",
    "        Url https://www.airantares.ro/cazare/in-Paris/Franta/beaugrenelle-eiffel-tour-3-stars-paris-franta/\n",
    "        Title b'Beaugrenelle Tour Eiffel, Paris, Franta'\n",
    "        Image https://i.travelapi.com/hotels/2000000/1070000/1063000/1062936/c5a49732.jpg\n",
    "        \"\"\"\n",
    "\n",
    "        if (obj[\"width\"] * obj[\"height\"]) > 307200 and obj[\"image\"].split(\".\")[-1].lower() in exts:\n",
    "            links.append(obj[\"image\"])\n",
    "\n",
    "    return links\n",
    "\n",
    "def links_from_ddg(topic, max_images=None, exts=[\"jpg\", \"png\", \"bmp\", \"jpeg\"]):\n",
    "    link_list = []\n",
    "\n",
    "    url = 'https://duckduckgo.com/' \n",
    "    params = {'q': topic} \n",
    "\n",
    "    #   First make a request to above URL, and parse out the 'vqd'\n",
    "    #   This is a special token, which should be used in the subsequent request\n",
    "    res = requests.post(url, data=params)\n",
    "    searchObj = re.search(r'vqd=([\\d-]+)\\&', res.text, re.M|re.I) \n",
    "\n",
    "    if not searchObj:\n",
    "        # Token parsing failed\n",
    "        return -1 \n",
    "\n",
    "    headers = {\n",
    "        'authority': 'duckduckgo.com',\n",
    "        'accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'x-requested-with': 'XMLHttpRequest',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'referer': 'https://duckduckgo.com/',\n",
    "        'accept-language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('l', 'us-en'),\n",
    "        ('o', 'json'),\n",
    "        ('q', topic),\n",
    "        ('vqd', searchObj.group(1)),\n",
    "        ('f', ',,,'),\n",
    "        ('p', '1'),\n",
    "        ('v7exp', 'a'),\n",
    "    )\n",
    "\n",
    "    requestUrl = url + \"i.js\"\n",
    "\n",
    "    print(\"Scraping links from DuckDuckGo\")\n",
    "    tq = tqdm(total=max_images)\n",
    "    link_count = 0\n",
    "    while True:\n",
    "        while True:\n",
    "            try:\n",
    "                res = requests.get(requestUrl, headers=headers, params=params)\n",
    "                data = json.loads(res.text)\n",
    "                break\n",
    "            except ValueError:\n",
    "                # Hitting Url Failure - Sleep and Retry\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "        links = extract_json(data[\"results\"], exts)\n",
    "        for link in links:\n",
    "            if max_images and link_count != max_images:\n",
    "                link_list += [link]\n",
    "                link_count += 1\n",
    "                tq.update(1)\n",
    "            else:\n",
    "                return link_list\n",
    "            \n",
    "\n",
    "        if \"next\" not in data:\n",
    "            # No next page\n",
    "            return link_list\n",
    "\n",
    "        requestUrl = url + data[\"next\"]"
   ]
  },
  {
   "source": [
    "## Link scraping (Flickr)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_from_flickr(topic, max_images):\n",
    "    KEY = '88a8660edd2e770b1b00e878af174879'\n",
    "    SECRET = 'f3063c276e3ad859'\n",
    "\n",
    "    SIZES = [\"url_o\", \"url_k\", \"url_h\", \"url_l\", \"url_c\"]  # in order of preference\n",
    "\n",
    "    \"\"\"\n",
    "    - url_o: Original (4520 × 3229)\n",
    "    - url_k: Large 2048 (2048 × 1463)\n",
    "    - url_h: Large 1600 (1600 × 1143)\n",
    "    - url_l=: Large 1024 (1024 × 732)\n",
    "    - url_c: Medium 800 (800 × 572)\n",
    "    - url_z: Medium 640 (640 × 457)\n",
    "    - url_m: Medium 500 (500 × 357)\n",
    "    - url_n: Small 320 (320 × 229)\n",
    "    - url_s: Small 240 (240 × 171)\n",
    "    - url_t: Thumbnail (100 × 71)\n",
    "    - url_q: Square 150 (150 × 150)\n",
    "    - url_sq: Square 75 (75 × 75)\n",
    "    \"\"\"\n",
    "    \n",
    "    extras = ','.join(SIZES)\n",
    "    flickr = flickrapi.FlickrAPI(KEY, SECRET)\n",
    "    photos = flickr.walk(text=topic,  # it will search by image title and image tags\n",
    "                            extras=extras,  # get the urls for each size we want\n",
    "                            privacy_filter=1,  # search only for public photos\n",
    "                            per_page=50,\n",
    "                            sort='relevance')  # we want what we are looking for to appear first\n",
    "    counter, urls = 0, []\n",
    "\n",
    "    print(\"Scraping links from Flickr\")\n",
    "    tq = tqdm(total = max_images)\n",
    "    for photo in photos:\n",
    "        if counter < max_images:\n",
    "            for i in range(len(SIZES)):  # makes sure the loop is done in the order we want\n",
    "                url = photo.get(SIZES[i])\n",
    "                if url:  # if url is None try with the next size\n",
    "                    urls.append(url)\n",
    "                    counter += 1\n",
    "                    tq.update(1)\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "source": [
    "## Link scraping (Bing)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_from_bing(topic, max_images, exts=[\"jpg\", \"png\", \"bmp\", \"jpeg\"], adult=\"off\", bing_filter=\"filterui:imagesize-custom_640_480\"):\n",
    "    links = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0'}\n",
    "    image_counter = 0\n",
    "    page_counter = 0\n",
    "\n",
    "    print(\"Scraping links from Flickr\")\n",
    "    tq = tqdm(total = max_images)\n",
    "    while image_counter < max_images:\n",
    "        # Parse the page source and download pics\n",
    "        request_url = 'https://www.bing.com/images/async?q=' + urllib.parse.quote_plus(topic) \\\n",
    "                        + '&first=' + str(page_counter) + '&count=' + str(max_images) \\\n",
    "                        + '&adlt=' + adult + '&qft=' + bing_filter\n",
    "        request = urllib.request.Request(request_url, None, headers=headers)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        html = response.read().decode('utf8')\n",
    "        found_links = re.findall('murl&quot;:&quot;(.*?)&quot;', html)\n",
    "\n",
    "        for link in found_links:\n",
    "            if image_counter < max_images:\n",
    "                try:\n",
    "                    path = urllib.parse.urlsplit(link).path\n",
    "                    filename = posixpath.basename(path).split('?')[0]\n",
    "                    file_type = filename.split(\".\")[-1]\n",
    "                    link = link[:link.index(\".\"+file_type)]+\".\"+file_type\n",
    "                    if file_type.lower() in exts:\n",
    "                        links.append(link)\n",
    "                        image_counter += 1\n",
    "                        tq.update(1)\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        page_counter += 1\n",
    "    tq.close()\n",
    "    return links\n"
   ]
  },
  {
   "source": [
    "## Download Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_download(item):\n",
    "    link = item[\"link\"]\n",
    "    folder = item[\"folder\"]\n",
    "    service = item[\"service\"]\n",
    "    link_hash = str(hashlib.md5(link.encode(\"utf-8\")).hexdigest())\n",
    "    ext = link.split(\".\")[-1].lower()\n",
    "    fname = \"image_{}.{}\".format(link_hash, ext)\n",
    "    \n",
    "    path = os.path.join(folder, fname)\n",
    "\n",
    "    if not os.path.isfile(path):\n",
    "        myfile = None\n",
    "        if service == \"ddg\":\n",
    "            myfile = requests.get(link, allow_redirects=True)\n",
    "            open(path, 'wb').write(myfile.content)\n",
    "        elif service == \"flickr\":\n",
    "            myfile = requests.get(link, stream=True)\n",
    "            open(path, 'wb').write(myfile.content)\n",
    "        elif service == \"bing\":\n",
    "            wget.download(link, path)\n",
    "\n",
    "def download(links, folder, service=\"flickr\"):\n",
    "    items = []\n",
    "    for link in links:\n",
    "        items.append({\"link\": link, \"folder\": folder, \"service\": service})\n",
    "    thread_it(thread_download, items, WORKERS=None)"
   ]
  },
  {
   "source": [
    "## Download definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task\n",
    "compare_dir = \"compare_set/\"\n",
    "data_dir = \"images/\"\n",
    "\n",
    "dirs = [compare_dir, data_dir]\n",
    "for path in dirs:\n",
    "    create_folder(path)\n",
    "\n",
    "CPUs = multiprocessing.cpu_count()\n",
    "# It will be double, since downloading from both Bing and Flickr.\n",
    "data_num = 300\n",
    "compare_num = 100\n",
    "topic = \"eiffel tower\""
   ]
  },
  {
   "source": [
    "## Download the sets\n",
    "#### Download the comparison set and count number of downloaded files "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download comparison set\n",
    "exts = [\"jpg\", \"jpeg\"]\n",
    "links = links_from_ddg(topic, max_images = compare_num, exts = exts)\n",
    "download(links, compare_dir, service=\"ddg\")\n",
    "file_num = len(glob.glob(compare_dir+\"*\", recursive=True))\n",
    "print(\"Downloaded {} images for the comparison set\".format(file_num))"
   ]
  },
  {
   "source": [
    "#### Download the data set and count number of downloaded files "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloaded 492 images for the data set\n"
    }
   ],
   "source": [
    "# Download data set from Flickr\n",
    "links = links_from_flickr(topic, max_images=data_num)\n",
    "download(links, data_dir, service=\"flickr\")\n",
    "file_num = len(glob.glob(data_dir+\"*\", recursive=True))\n",
    "print(\"Downloaded {} images for the data set\".format(file_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scraping links from Flickr\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value=&#39;&#39;)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7da1805b7cd843c5a0b52400efb32bf6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n[&#39;http://justinekibler.files.wordpress.com/2014/06/eiffel-tower-night.jpg&#39;, &#39;https://www.pixelstalk.net/wp-content/uploads/2015/05/eiffel-tower-night.jpg&#39;, &#39;http://creativetravelguide.com/wp-content/uploads/2016/11/eiffel-tower-header.jpg&#39;, &#39;http://www.reidsfrance.com/site/assets/files/5978/eiffel-tower-at-night.jpg&#39;, &#39;http://2.bp.blogspot.com/-8XPuDaEb4uY/UqsegNDSxfI/AAAAAAAB5NI/mv1On3rGARw/s1600/Eiffel+Tower+Landscape+Wallpaper.jpg&#39;, &#39;https://new7wonders.com/app/uploads/sites/3/2016/09/ET_night_ls_central2_V-1920x1440.jpg&#39;, &#39;http://upload.wikimedia.org/wikipedia/commons/e/e2/Eiffel_tower_at_night_WLM.JPG&#39;, &#39;https://travelwithsandra.files.wordpress.com/2013/03/14915042-the-eiffel-tower-and-trocadero-fountain-in-paris-france.jpg&#39;, &#39;http://3.bp.blogspot.com/-DDBsWZ1hOE0/UGqxiQ9m6DI/AAAAAAAAGpk/PJwiefRjc-s/s1600/Paris___Eiffel_Tower_by_tariyoko.jpg&#39;, &#39;http://momvoyage.hilton.com/wp-content/uploads/2014/06/DSCN1915.jpg&#39;, &#39;https://meets.com/articles/wp-content/uploads/2014/09/Eiffel-Tower-in-Paris-France.jpg&#39;, &#39;http://sparkhistory.com/wp-content/uploads/2018/04/paris-france-eiffel-tower-wideview-SparkHistory.jpg&#39;, &#39;http://sparkhistory.com/wp-content/uploads/2018/04/paris-france-eiffel-tower-from-below1-SparkHistory.jpg&#39;, &#39;https://maninamsterdam.files.wordpress.com/2012/04/eiffel2.jpg&#39;, &#39;https://www.pixelstalk.net/wp-content/uploads/2016/06/Beautiful-eiffel-tower-wallpaper.jpg&#39;, &#39;http://3.bp.blogspot.com/-r9Zw-yl5aVA/Ubcp4PFOCdI/AAAAAAAATng/1U3TUiiAfDM/s1600/eiffel+tower+11.jpg&#39;, &#39;https://upload.wikimedia.org/wikipedia/commons/0/01/Eiffel_Tower%2C_Paris_17_September_2010.jpg&#39;, &#39;https://aboutandabroad.files.wordpress.com/2015/06/eiffel-tower-at-dusk.jpg&#39;, &#39;http://www.historylines.net/img/culture/architecture/Big/eiffel_tower_paris.jpg&#39;, &#39;https://www.jasminealley.com/wp-content/uploads/2019/07/trocadero-sloped-wall-with-eiffel-tower-view.jpg&#39;]\n15000\n"
    }
   ],
   "source": [
    "# Download data set from Bing\n",
    "links = links_from_bing(topic, max_images=data_num)\n",
    "download(links, data_dir, service=\"bing\")\n",
    "file_num_new = len(glob.glob(data_dir+\"*\", recursive=True))\n",
    "print(\"Downloaded {} images for the data set\".format(file_num_new-file_num))"
   ]
  },
  {
   "source": [
    "## Post-Download Cleaning\n",
    "#### Rename all files to \".jpg\" file format, remove anything not \".jpg\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_rename(image):\n",
    "    fname = image.split(\".\")[-2]\n",
    "    ext = image.split(\".\")[-1]\n",
    "    if ext == \"jpeg\":\n",
    "        shutil.move(image, fname+\".jpg\")\n",
    "    elif ext == \"jpg\":\n",
    "        return\n",
    "    else:\n",
    "        os.remove(image)\n",
    "        \n",
    "def images_rename(path):\n",
    "    files = glob.glob(path+\"*.*\", recursive=True)\n",
    "    thread_it(img_rename, files)\n",
    "\n",
    "images_rename(compare_dir)\n",
    "images_rename(data_dir)"
   ]
  },
  {
   "source": [
    "#### Detect file corruption"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verify_thread(image):\n",
    "    try:\n",
    "        img = io.imread(image)\n",
    "    except Exception as e:\n",
    "        return (image, e)\n",
    "\n",
    "def verify_images(path):\n",
    "    files = glob.glob(path+\"*.jpg\", recursive=True)\n",
    "    corrupt = thread_it_return(verify_thread, files)\n",
    "    return corrupt\n",
    "\n",
    "corrupt_compare = verify_images(compare_dir)\n",
    "print(\"Compare directory:\")\n",
    "pprint(corrupt_compare)\n",
    "\n",
    "corrupt_data = verify_images(data_dir)\n",
    "print(\"Data directory:\")\n",
    "pprint(corrupt_data)"
   ]
  },
  {
   "source": [
    "#### Remove corrupt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_remove_thread(item):\n",
    "    os.remove(item[0])\n",
    "\n",
    "def remove_items(items):\n",
    "    thread_it(img_remove_thread, items)\n",
    "    \n",
    "remove_items(corrupt_compare)\n",
    "remove_items(corrupt_data)"
   ]
  },
  {
   "source": [
    "#### Checking minimum image size"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def res_check_thread(image):\n",
    "    img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Get image height and width\n",
    "    #height, width, channels = img.shape\n",
    "    height, width = img.shape\n",
    "\n",
    "    # Count maximum resolution\n",
    "    val = width * height\n",
    "    \n",
    "    # Show warning if problem\n",
    "    # 640*480\n",
    "    if val < 307200:\n",
    "        return image\n",
    "\n",
    "\n",
    "def get_under_res(path):\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "    under_res = thread_it_return(res_check_thread, files)\n",
    "    \n",
    "    return under_res\n",
    "\n",
    "print(\"Checking resolution for images (Compare):\")\n",
    "compare_under_res = get_under_res(compare_dir)\n",
    "pprint(compare_under_res)\n",
    "print(\"Checking resolution for images (Data):\")\n",
    "data_under_res = get_under_res(data_dir)\n",
    "pprint(data_under_res)\n",
    "\n",
    "print(\"Deleting found items:\")\n",
    "remove_items(compare_under_res)\n",
    "remove_items(data_under_res)\n"
   ]
  },
  {
   "source": [
    "# Ensure the images in the \"compare\" directory contain the subject and aren't blurred."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Then proceed, to \"Honours\" notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}