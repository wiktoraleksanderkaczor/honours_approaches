{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbasecondad5f027570b844e40ba6c6d5b4044b6f5",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import wget\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import shutil\n",
    "import urllib\n",
    "import hashlib\n",
    "import requests\n",
    "import flickrapi\n",
    "import imagehash\n",
    "import posixpath\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "source": [
    "## Helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def thread_it(thread_function, my_list, tq=True, WORKERS=None):\n",
    "    # Set worker number to CPU count\n",
    "    if not WORKERS:\n",
    "        WORKERS = multiprocessing.cpu_count()\n",
    "    \n",
    "    if tq:\n",
    "        tq = tqdm(total=len(my_list))\n",
    "    \n",
    "    # Separate into chunks and execute threaded\n",
    "    thread_list = chunks(my_list, WORKERS)\n",
    "    for chunk in thread_list:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "            for item in chunk:\n",
    "                executor.submit(thread_function, item)\n",
    "                if tq:\n",
    "                    tq.update(1)\n",
    "    tq.close()\n",
    "\n",
    "\n",
    "def thread_it_return(thread_function, my_list, tq=True, WORKERS=None):\n",
    "    # Set worker number to CPU count\n",
    "    if not WORKERS:\n",
    "        WORKERS = multiprocessing.cpu_count()\n",
    "    \n",
    "    if tq:\n",
    "        tq = tqdm(total=len(my_list))\n",
    "        \n",
    "    results = []\n",
    "    # Separate into chunks and execute threaded\n",
    "    thread_list = chunks(my_list, WORKERS)\n",
    "    for chunk in thread_list:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "            for item in chunk:\n",
    "                future = executor.submit(thread_function, item)\n",
    "                \n",
    "                return_value = future.result()\n",
    "                if return_value != None:\n",
    "                    results.append(return_value)\n",
    "                    \n",
    "                if tq:\n",
    "                    tq.update(1)\n",
    "    \n",
    "    tq.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_folder(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def move_to(file_list, dest):\n",
    "    tq = tqdm(total=len(file_list))\n",
    "    exception_flag = False\n",
    "    for item in file_list:\n",
    "        try:\n",
    "            shutil.move(item, dest)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            exception_flag = True\n",
    "        tq.update(1)\n",
    "    tq.close()\n",
    "    return exception_flag"
   ]
  },
  {
   "source": [
    "## Link scraping (DuckDuckGo)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(objs, exts):\n",
    "    links = []\n",
    "    for obj in objs:\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Width {0}, Height {1}\".format(obj[\"width\"], obj[\"height\"]))\n",
    "        print(\"Thumbnail {0}\".format(obj[\"thumbnail\"]))\n",
    "        print(\"Url {0}\".format(obj[\"url\"]))\n",
    "        print(\"Title {0}\".format(obj[\"title\"].encode('utf-8')))\n",
    "        print(\"Image {0}\".format(obj[\"image\"]))\n",
    "        \n",
    "        -- EXAMPLE OUTPUT --\n",
    "        Width 3840, Height 2560\n",
    "        Thumbnail https://tse1.mm.bing.net/th?id=OIF.BrhofaJg5Fx2yl9jrBBQLQ&pid=Api\n",
    "        Url https://www.airantares.ro/cazare/in-Paris/Franta/beaugrenelle-eiffel-tour-3-stars-paris-franta/\n",
    "        Title b'Beaugrenelle Tour Eiffel, Paris, Franta'\n",
    "        Image https://i.travelapi.com/hotels/2000000/1070000/1063000/1062936/c5a49732.jpg\n",
    "        \"\"\"\n",
    "\n",
    "        if (obj[\"width\"] * obj[\"height\"]) > 307200 and obj[\"image\"].split(\".\")[-1].lower() in exts:\n",
    "            links.append(obj[\"image\"])\n",
    "\n",
    "    return links\n",
    "\n",
    "def links_from_ddg(topic, max_images=None, exts=[\"jpg\", \"png\", \"jpeg\"]):\n",
    "    link_list = []\n",
    "\n",
    "    url = 'https://duckduckgo.com/' \n",
    "    params = {'q': topic} \n",
    "\n",
    "    #   First make a request to above URL, and parse out the 'vqd'\n",
    "    #   This is a special token, which should be used in the subsequent request\n",
    "    res = requests.post(url, data=params)\n",
    "    searchObj = re.search(r'vqd=([\\d-]+)\\&', res.text, re.M|re.I) \n",
    "\n",
    "    if not searchObj:\n",
    "        # Token parsing failed\n",
    "        return -1 \n",
    "\n",
    "    headers = {\n",
    "        'authority': 'duckduckgo.com',\n",
    "        'accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'x-requested-with': 'XMLHttpRequest',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'referer': 'https://duckduckgo.com/',\n",
    "        'accept-language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('l', 'us-en'),\n",
    "        ('o', 'json'),\n",
    "        ('q', topic),\n",
    "        ('vqd', searchObj.group(1)),\n",
    "        ('f', ',,,'),\n",
    "        ('p', '1'),\n",
    "        ('v7exp', 'a'),\n",
    "    )\n",
    "\n",
    "    requestUrl = url + \"i.js\"\n",
    "\n",
    "    print(\"Scraping links from DuckDuckGo\")\n",
    "    tq = tqdm(total=max_images)\n",
    "    link_count = 0\n",
    "    while True:\n",
    "        while True:\n",
    "            try:\n",
    "                res = requests.get(requestUrl, headers=headers, params=params)\n",
    "                data = json.loads(res.text)\n",
    "                break\n",
    "            except ValueError:\n",
    "                # Hitting Url Failure - Sleep and Retry\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "        links = extract_json(data[\"results\"], exts)\n",
    "        for link in links:\n",
    "            if max_images and link_count != max_images:\n",
    "                link_list += [link]\n",
    "                link_count += 1\n",
    "                tq.update(1)\n",
    "            else:\n",
    "                return link_list\n",
    "            \n",
    "\n",
    "        if \"next\" not in data:\n",
    "            # No next page\n",
    "            return link_list\n",
    "\n",
    "        requestUrl = url + data[\"next\"]"
   ]
  },
  {
   "source": [
    "## Link scraping (Flickr)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_from_flickr(topic, max_images):\n",
    "    KEY = '88a8660edd2e770b1b00e878af174879'\n",
    "    SECRET = 'f3063c276e3ad859'\n",
    "\n",
    "    SIZES = [\"url_o\", \"url_k\", \"url_h\", \"url_l\", \"url_c\"]  # in order of preference\n",
    "\n",
    "    \"\"\"\n",
    "    - url_o: Original (4520 × 3229)\n",
    "    - url_k: Large 2048 (2048 × 1463)\n",
    "    - url_h: Large 1600 (1600 × 1143)\n",
    "    - url_l=: Large 1024 (1024 × 732)\n",
    "    - url_c: Medium 800 (800 × 572)\n",
    "    - url_z: Medium 640 (640 × 457)\n",
    "    - url_m: Medium 500 (500 × 357)\n",
    "    - url_n: Small 320 (320 × 229)\n",
    "    - url_s: Small 240 (240 × 171)\n",
    "    - url_t: Thumbnail (100 × 71)\n",
    "    - url_q: Square 150 (150 × 150)\n",
    "    - url_sq: Square 75 (75 × 75)\n",
    "    \"\"\"\n",
    "    \n",
    "    extras = ','.join(SIZES)\n",
    "    flickr = flickrapi.FlickrAPI(KEY, SECRET)\n",
    "    photos = flickr.walk(text=topic,  # it will search by image title and image tags\n",
    "                            extras=extras,  # get the urls for each size we want\n",
    "                            privacy_filter=1,  # search only for public photos\n",
    "                            per_page=50,\n",
    "                            sort='relevance')  # we want what we are looking for to appear first\n",
    "    counter, urls = 0, []\n",
    "\n",
    "    print(\"Scraping links from Flickr\")\n",
    "    tq = tqdm(total = max_images)\n",
    "    for photo in photos:\n",
    "        if counter < max_images:\n",
    "            for i in range(len(SIZES)):  # makes sure the loop is done in the order we want\n",
    "                url = photo.get(SIZES[i])\n",
    "                if url:  # if url is None try with the next size\n",
    "                    urls.append(url)\n",
    "                    counter += 1\n",
    "                    tq.update(1)\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "source": [
    "## Link scraping (Bing)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_from_bing(topic, max_images, exts=[\"jpg\", \"png\", \"jpeg\"], adult=\"off\", bing_filter=\"filterui:imagesize-custom_640_480\"):\n",
    "    links = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0'}\n",
    "    image_counter = 0\n",
    "    page_counter = 0\n",
    "\n",
    "    tq = tqdm(total = max_images)\n",
    "    while image_counter < max_images:\n",
    "        # Parse the page source and download pics\n",
    "        request_url = 'https://www.bing.com/images/async?q=' + urllib.parse.quote_plus(topic) \\\n",
    "                        + '&first=' + str(page_counter) + '&count=' + str(max_images) \\\n",
    "                        + '&adlt=' + adult + '&qft=' + bing_filter\n",
    "        request = urllib.request.Request(request_url, None, headers=headers)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        html = response.read().decode('utf8')\n",
    "        found_links = re.findall('murl&quot;:&quot;(.*?)&quot;', html)\n",
    "\n",
    "        for link in found_links:\n",
    "            if image_counter < max_images:\n",
    "                try:\n",
    "                    path = urllib.parse.urlsplit(link).path\n",
    "                    filename = posixpath.basename(path).split('?')[0]\n",
    "                    file_type = filename.split(\".\")[-1]\n",
    "                    link = link[:link.index(\".\"+file_type)]+\".\"+file_type\n",
    "                    if file_type.lower() in exts:\n",
    "                        links.append(link)\n",
    "                        image_counter += 1\n",
    "                        tq.update(1)\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        page_counter += 1\n",
    "    tq.close()\n",
    "    return links\n"
   ]
  },
  {
   "source": [
    "## Download Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_download(item):\n",
    "    link = item[\"link\"]\n",
    "    folder = item[\"folder\"]\n",
    "    service = item[\"service\"]\n",
    "    link_hash = str(hashlib.md5(link.encode(\"utf-8\")).hexdigest())\n",
    "    ext = link.split(\".\")[-1].lower()\n",
    "    fname = \"image-{}.{}\".format(link_hash, ext)\n",
    "    \n",
    "    path = os.path.join(folder, fname)\n",
    "\n",
    "    if not os.path.isfile(path):\n",
    "        myfile = None\n",
    "        if service == \"ddg\":\n",
    "            myfile = requests.get(link, allow_redirects=True, timeout=0.5)\n",
    "            open(path, 'wb').write(myfile.content)\n",
    "        elif service == \"flickr\":\n",
    "            myfile = requests.get(link, stream=True, timeout=0.5)\n",
    "            open(path, 'wb').write(myfile.content)\n",
    "        elif service == \"bing\":\n",
    "            myfile = requests.get(link, timeout=0.5)\n",
    "            open(path, 'wb').write(myfile.content)\n",
    "            #wget.download(link, path)\n",
    "\n",
    "def download(links, folder, service=\"flickr\"):\n",
    "    items = []\n",
    "    for link in links:\n",
    "        items.append({\"link\": link, \"folder\": folder, \"service\": service})\n",
    "    thread_it(thread_download, items, WORKERS=None)"
   ]
  },
  {
   "source": [
    "## Download definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task\n",
    "compare_dir = \"compare_set/\"\n",
    "data_dir = \"images/\"\n",
    "blurry = \"blurry/\"\n",
    "pickles = \"pickles/\"\n",
    "duplicates = \"duplicates/\"\n",
    "histogram_dir = \"histogram_check/\"\n",
    "too_small = \"too_small/\"\n",
    "\n",
    "dirs = [compare_dir, data_dir, blurry, pickles, duplicates, histogram_dir, too_small]\n",
    "for path in dirs:\n",
    "    create_folder(path)\n",
    "\n",
    "CPUs = multiprocessing.cpu_count()\n",
    "# It will be double, since downloading from both Bing and Flickr.\n",
    "bing_data_num = 5000\n",
    "flickr_data_num = 5000\n",
    "compare_num = 1000\n",
    "topic = \"eiffel tower\""
   ]
  },
  {
   "source": [
    "## Download the sets\n",
    "#### Download the comparison set and count number of downloaded files "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download comparison set\n",
    "exts = [\"jpg\", \"jpeg\", \"png\"]\n",
    "links = links_from_ddg(topic, max_images = compare_num, exts = exts)\n",
    "download(links, compare_dir, service=\"ddg\")\n",
    "file_num = len(glob.glob(compare_dir+\"*\", recursive=True))\n",
    "print(\"Downloaded {} images for the comparison set\".format(file_num))"
   ]
  },
  {
   "source": [
    "#### Download the data set and count number of downloaded files "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download data set from Flickr\n",
    "links = links_from_flickr(topic, max_images=flickr_data_num)\n",
    "download(links, data_dir, service=\"flickr\")\n",
    "file_num = len(glob.glob(data_dir+\"*\", recursive=True))\n",
    "print(\"Downloaded {} images for the data set\".format(file_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download data set from Bing\n",
    "links = links_from_bing(topic, max_images=bing_data_num)\n",
    "download(links, data_dir, service=\"bing\")\n",
    "file_num_new = len(glob.glob(data_dir+\"*\", recursive=True))\n",
    "print(\"Downloaded {} images for the data set\".format(file_num_new-file_num))"
   ]
  },
  {
   "source": [
    "## Post-Download Cleaning\n",
    "#### Rename all \".jpeg\" to \".jpg\" file format."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_rename(image):\n",
    "    fname = image.split(\".\")[-2]\n",
    "    ext = image.split(\".\")[-1]\n",
    "    if ext == \"jpeg\":\n",
    "        shutil.move(image, fname+\".jpg\")\n",
    "    elif ext == \"jpg\":\n",
    "        return\n",
    "    else:\n",
    "        return\n",
    "                \n",
    "def images_rename(path):\n",
    "    files = glob.glob(path+\"*.*\", recursive=True)\n",
    "    thread_it(img_rename, files)\n",
    "\n",
    "images_rename(compare_dir)\n",
    "images_rename(data_dir)"
   ]
  },
  {
   "source": [
    "#### Detect file corruption"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the JPEG is valid, if not remove.\n",
    "!jpeginfo -cd **/*.jpg;\n",
    "# Check the PNG is valid, if not remove.\n",
    "!pngcheck -fq **/*.png | cut -d \" \" -f2 | xargs rm;\n",
    "\n",
    "#Convert all formats to JPEG.\n",
    "!mogrify -format jpg **/*.png;\n",
    "\n",
    "# Remove converted files.\n",
    "!rm **/*.png"
   ]
  },
  {
   "source": [
    "#### Checking minimum image size"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def res_check_thread(image):\n",
    "    img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Get image height and width\n",
    "    #height, width, channels = img.shape\n",
    "    height, width = img.shape\n",
    "\n",
    "    # Count maximum resolution\n",
    "    val = width * height\n",
    "    \n",
    "    # Show warning if problem\n",
    "    # 640*480\n",
    "    if val < 307200:\n",
    "        return image\n",
    "\n",
    "\n",
    "def get_under_res(path):\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "    under_res = thread_it_return(res_check_thread, files)\n",
    "    \n",
    "    return under_res\n",
    "\n",
    "print(\"Checking resolution for images (Compare):\")\n",
    "compare_under_res = get_under_res(compare_dir)\n",
    "pprint(compare_under_res)\n",
    "\n",
    "print(\"Checking resolution for images (Data):\")\n",
    "data_under_res = get_under_res(data_dir)\n",
    "pprint(data_under_res)\n",
    "\n",
    "print(\"Moving found items\")\n",
    "if not move_to(compare_under_res, too_small):\n",
    "    del(compare_under_res)\n",
    "if not move_to(data_under_res, too_small):\n",
    "    del(data_under_res)"
   ]
  },
  {
   "source": [
    "#### Get the images in the sets that are more blurry than a given threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def too_blurry_thread(item):\n",
    "    image = item[\"image\"]\n",
    "    threshold = item[\"threshold\"]\n",
    "    img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "    val = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "    if val < threshold:\n",
    "        return image\n",
    "                \n",
    "def get_too_blurry(path, threshold):\n",
    "    files = glob.glob(path+\"*.jpg\")\n",
    "    items = []\n",
    "    for image in files:\n",
    "        items.append({\"image\": image, \"threshold\": threshold})\n",
    "    too_blurry = thread_it_return(too_blurry_thread, items)\n",
    "    \n",
    "    print(\"{} out of {} images are blurry\".format(len(too_blurry), len(files)))\n",
    "    return too_blurry\n",
    "\n",
    "#https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\n",
    "# A constant of 200, it does pretty good!\n",
    "too_blurry_compare = get_too_blurry(compare_dir, 200)\n",
    "too_blurry_data = get_too_blurry(data_dir, 200)"
   ]
  },
  {
   "source": [
    "#### Move those images to blurry folder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not move_to(too_blurry_compare, blurry):\n",
    "    del(too_blurry_compare)\n",
    "\n",
    "if not move_to(too_blurry_data, blurry):\n",
    "    del(too_blurry_data)"
   ]
  },
  {
   "source": [
    "#### Hash images to check for duplicates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_thread(item):\n",
    "    fname = item[\"image\"]\n",
    "    hashes = item[\"hashes\"]\n",
    "    if fname not in hashes.keys():\n",
    "        hashes[fname] = imagehash.dhash(Image.open(fname))\n",
    "\n",
    "def compute_img_hashes(path):\n",
    "    items = []\n",
    "    files = glob.glob(path+\"*.jpg\", recursive=True)\n",
    "    hash_file = pickles+path[:-1]+\"_hashes.pickle\"\n",
    "    if os.path.isfile(hash_file):\n",
    "        hashes = pickle.load(open(hash_file, \"rb\"))\n",
    "    else:\n",
    "        hashes = {}\n",
    "    for image in files:\n",
    "        items.append({\"image\": image, \"hashes\": hashes})\n",
    "    \n",
    "    thread_it(hash_thread, items)\n",
    "    pickle.dump(hashes, open(hash_file, \"wb\"))\n",
    "\n",
    "compute_img_hashes(compare_dir)\n",
    "compute_img_hashes(data_dir)"
   ]
  },
  {
   "source": [
    "#### Compute hash distances for each image to each image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_distance_thread(item):\n",
    "    image = item[\"image\"]\n",
    "    hashes = item[\"hashes\"]\n",
    "    distances = item[\"distances\"]\n",
    "    hash1 = hashes[image]\n",
    "\n",
    "    for hash2name in hashes.keys():\n",
    "        # If not same image, and not already done for that image\n",
    "        if hash2name != image and hash2name not in distances[image].keys():\n",
    "            hash2 = hashes[hash2name]\n",
    "            distances[image][hash2name] = hash1 - hash2\n",
    "            \n",
    "def compute_hash_distance(path):\n",
    "    hashes_path = pickles+path[:-1]+\"_hashes.pickle\"\n",
    "    distances_path = pickles+path[:-1]+\"_distances.pickle\"\n",
    "    files = glob.glob(path+\"*.jpg\", recursive=True)\n",
    "    if os.path.isfile(hashes_path):\n",
    "        hashes = pickle.load(open(hashes_path, \"rb\"))\n",
    "    else:\n",
    "        print(\"No hash file detected, cannot continue.\")\n",
    "        return\n",
    "\n",
    "    if os.path.isfile(distances_path):\n",
    "        distances = pickle.load(open(distances_path, \"rb\"))\n",
    "    else:\n",
    "        distances = {}\n",
    "    \n",
    "    for image in files:\n",
    "        distances[image] = {}\n",
    "\n",
    "    items = []\n",
    "    for image in files:\n",
    "        items.append({\"image\": image, \"hashes\": hashes, \"distances\": distances})\n",
    "\n",
    "    thread_it(hash_distance_thread, items)\n",
    "    pickle.dump(distances, open(distances_path, \"wb\"))\n",
    "\n",
    "compute_hash_distance(compare_dir)\n",
    "compute_hash_distance(data_dir)"
   ]
  },
  {
   "source": [
    "#### Check for duplicates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicate_images(path, threshold=10):\n",
    "    distances_path = pickles+path[:-1]+\"_distances.pickle\"\n",
    "    if os.path.isfile(distances_path):\n",
    "        distances = pickle.load(open(distances_path, \"rb\"))\n",
    "    else:\n",
    "        print(\"No distances file detected, cannot continue.\")\n",
    "        return\n",
    "\n",
    "    files = glob.glob(path+\"*.jpg\", recursive=True)\n",
    "    dup, close = [], []\n",
    "    \n",
    "    for path in files:\n",
    "        distances_item = distances[path]\n",
    "        img = path.split(\".\")[-2]+\".jpg\"\n",
    "        \n",
    "        for key, val in distances_item.items():\n",
    "            if val == 0:\n",
    "                if key not in dup and path not in dup:\n",
    "                    dup.append(key)\n",
    "            elif val < threshold:\n",
    "                if key not in close and path not in close:\n",
    "                    close.append(key)\n",
    "                \n",
    "    return dup, close\n",
    "\n",
    "compare_dup, compare_close = get_duplicate_images(compare_dir, threshold=5)\n",
    "data_dup, data_close = get_duplicate_images(data_dir, threshold=5)\n",
    "\n",
    "print(\"Compare duplicates:\")\n",
    "pprint(compare_dup)\n",
    "print(\"Compare close:\")\n",
    "pprint(compare_close)\n",
    "\n",
    "print(\"Data duplicates:\")\n",
    "pprint(data_dup)\n",
    "print(\"Data close:\")\n",
    "pprint(data_close)\n",
    "\n",
    "print(\"Length of compare duplicates: \", len(compare_dup))\n",
    "print(\"Length of data duplicates: \", len(data_dup))\n",
    "print(\"Length of compare close: \", len(compare_close))\n",
    "print(\"Length of data close: \", len(data_close))\n",
    "\n",
    "show = False\n",
    "if show:\n",
    "    print(\"DUPLICATES IN COMPARE:\")\n",
    "    for item in compare_dup:\n",
    "        show_img_by_path(item)\n",
    "    print(\"DUPLICATES IN DATA:\")\n",
    "    for item in data_dup:\n",
    "        show_img_by_path(item)\n",
    "    print(\"CLOSE IN COMPARE:\")\n",
    "    for item in compare_close:\n",
    "        show_img_by_path(item)\n",
    "    print(\"CLOSE IN DATA:\")\n",
    "    for item in data_close:\n",
    "        show_img_by_path(item)"
   ]
  },
  {
   "source": [
    "#### Move duplicates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not move_to(compare_dup, duplicates):\n",
    "    del(compare_dup)\n",
    "if not move_to(data_dup, duplicates):\n",
    "    del(data_dup)"
   ]
  },
  {
   "source": [
    "### Histogram check for unrealistic colors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histSize = 256\n",
    "histRange = (0, 256) # the upper boundary i\n",
    "accumulate = False\n",
    "\n",
    "def histogram_thread(fi):\n",
    "    try:\n",
    "        image = cv2.imread(fi)\n",
    "        bgr_planes = cv2.split(image)\n",
    "        b_hist = cv2.calcHist(bgr_planes, [0], None, [histSize], histRange, accumulate=accumulate)\n",
    "        g_hist = cv2.calcHist(bgr_planes, [1], None, [histSize], histRange, accumulate=accumulate)\n",
    "        r_hist = cv2.calcHist(bgr_planes, [2], None, [histSize], histRange, accumulate=accumulate)\n",
    "        new = []\n",
    "        for b, g, r in zip(b_hist, g_hist, r_hist):\n",
    "            total = b + g + r     \n",
    "            new.append(total/3)\n",
    "\n",
    "        num_differing = 0\n",
    "\n",
    "        for b, g, r, total in zip(b_hist, g_hist, r_hist, new):\n",
    "            b_per = b / total\n",
    "            g_per = g / total\n",
    "            r_per = r / total\n",
    "            diff = False\n",
    "            if b_per > 1.15 or b_per < 0.85:\n",
    "                diff = True\n",
    "            if g_per > 1.15 or g_per < 0.85:\n",
    "                diff = True\n",
    "            if r_per > 1.15 or r_per < 0.85:\n",
    "                diff = True\n",
    "            if diff:\n",
    "                num_differing += 1\n",
    "        \n",
    "        return (fi, num_differing)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(fi, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_files = glob.glob(compare_dir+\"*.jpg\")\n",
    "data_files = glob.glob(data_dir+\"*.jpg\")\n",
    "\n",
    "num_differing_compare = thread_it_return(histogram_thread, compare_files)\n",
    "num_differing_data = thread_it_return(histogram_thread, data_files)\n",
    "\n",
    "# 200 seems to be a good constant\n",
    "compare_to_move = []\n",
    "compare_moved = 0\n",
    "for fi, num_differing in num_differing_compare:\n",
    "    if num_differing < 200:\n",
    "        compare_to_move.append(fi)\n",
    "        compare_moved += 1\n",
    "        print(fi, num_differing, \"moved\")\n",
    "\n",
    "data_to_move = []\n",
    "data_moved = 0\n",
    "for fi, num_differing in num_differing_data:\n",
    "    if num_differing < 200:\n",
    "        data_to_move.append(fi)\n",
    "        data_moved += 1\n",
    "        print(fi, num_differing, \"moved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not move_to(compare_to_move, histogram_dir):\n",
    "    del(compare_to_move)\n",
    "if not move_to(data_to_move, histogram_dir):\n",
    "    del(data_to_move)\n",
    "\n",
    "print(\"Moved {} out of {} in compare\".format(compare_moved, len(compare_files)))\n",
    "print(\"Moved {} out of {} in data\".format(data_moved, len(data_files)))"
   ]
  },
  {
   "source": [
    "# Ensure the images in the \"compare\" directory contain the subject and aren't blurred."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Then proceed, to \"Honours\" notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}